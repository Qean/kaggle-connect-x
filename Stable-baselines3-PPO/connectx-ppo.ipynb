{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A notebook containing PPO RL agent for ConnectX competition (rows = 6, columns = 7, inarow = 4).\n\nCreated using stable-baselines3.","metadata":{}},{"cell_type":"markdown","source":"# 1. Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip install stable-baselines3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-22T14:28:46.93306Z","iopub.execute_input":"2022-01-22T14:28:46.93338Z","iopub.status.idle":"2022-01-22T14:29:02.334088Z","shell.execute_reply.started":"2022-01-22T14:28:46.933295Z","shell.execute_reply":"2022-01-22T14:29:02.333269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Import global libraries\nDocumentation\n- OpenAI Gym: https://gym.openai.com/\n- NumPy: https://numpy.org/doc/stable/index.html\n- PyTorch: https://pytorch.org/\n- Stable-Baselines3: https://stable-baselines3.readthedocs.io/en/master/index.html\n- Kaggle Environments: https://github.com/Kaggle/kaggle-environments","metadata":{}},{"cell_type":"code","source":"import gym\nimport os\nimport sys\nimport inspect\nimport numpy as np\n\nimport torch as th\nimport torch.nn as nn\nth.set_printoptions(profile=\"full\")\n\nfrom tqdm.auto import tqdm\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_checker import check_env\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom stable_baselines3.common.monitor import Monitor, load_results\nfrom stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n# Disregard the gfootball error, it's a known issue\n# Relevant issue: https://github.com/Kaggle/kaggle-environments/issues/102\nfrom kaggle_environments import make, evaluate","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:29:02.336255Z","iopub.execute_input":"2022-01-22T14:29:02.336487Z","iopub.status.idle":"2022-01-22T14:29:04.26222Z","shell.execute_reply.started":"2022-01-22T14:29:02.336458Z","shell.execute_reply":"2022-01-22T14:29:04.261258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Create ConnectX Environment","metadata":{}},{"cell_type":"markdown","source":"## 3.1. Environment\nThe agent will use a custom OpenAI Gym environment made with the help of Kaggle ConnectX environment","metadata":{}},{"cell_type":"code","source":"LOG_DIR = \"log/\"\n\nclass Connect4(gym.Env):\n    \"\"\"\n    Connect 4 game environment based on OpenAI Gym standard and Kaggle helpers\n    \n    Attributes\n    ----------\n    env : kaggle_environment.Environment \n        Class representing the ConnectX environment\n    switch_prob : float\n        A number between 0.0 and 1.0 representing probability of switching which agent\n        will play first\n    agents : list of [str or func]\n        Agents that will use the environment\n    trainer : dict of {str : func}\n        Dictionary with ``reset()`` and ``step()`` functions which reset and\n        step through the game board\n    board_template : tuple of int\n        Template for the game board (dimensions)\n    board : numpy.ndarray of int\n        Current game board\n    reward_range : tuple of int (1, 0 -1)\n        Environment rewards\n    action_space : gym.spaces.Discrete\n        Object representing environment action space (columns from 0 to 6)\n    observation_space : gym.spaces.Box\n        Object representing environment observation space \n        (game board dimensions and possible values)\n    \"\"\"\n    def __init__(self, opponent = 'random', switch_prob = 0.5):\n        self.env = make('connectx', debug = False)\n        self.switch_prob = switch_prob\n        self.agents = [None, opponent] # The agent will train in place of None\n        self.trainer = self.env.train(self.agents)\n        config = self.env.configuration\n        # PyTorch Conv2d expect 4 dimensional data \n        # (nSamples x nChannels x Height x Width)\n        self.board_template = (1, config.rows, config.columns) \n        self.board = np.zeros(self.board_template, int)\n\n        # Define required gym fields\n        self.reward_range = (-1, 0, 1) # lose: -1, draw: 0, win: 1\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Box(\n            # ``low`` and ``high`` represent the possible values\n            low = 0,\n            high = 2,\n            shape = self.board_template,\n            dtype = int\n        )\n    \n    def switch_starting_positions(self):\n        self.agents = self.agents[::-1]\n        self.trainer = self.env.train(self.agents)\n\n    def step(self, action):\n        # Check for invalid moves\n        if self.board[0][0][int(action)] != 0:\n            reward, done, _ = -10, True, {}\n        else:\n            observation, reward, done, _ = self.trainer.step(int(action))\n            self.board = np.array(observation['board']).reshape(self.board_template)\n                        \n        return self.board, reward, done, _\n    \n    def reset(self):      \n        if np.random.random() < self.switch_prob:\n            self.switch_starting_positions()\n            \n        self.board = np.array(\n            self.trainer.reset()['board']\n        ).reshape(self.board_template)\n\n        return self.board\n\ntraining_env = Connect4('random')\n\n# Create directory for logging training information\nos.makedirs(LOG_DIR, exist_ok = True)\n\n# Add logging\ntraining_env = Monitor(training_env, LOG_DIR, allow_early_resets = True)\n\n# Vectorize environment (expected by stable-baselines3 algorithms)\ntraining_env = DummyVecEnv([lambda: training_env])","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-01-22T14:29:04.264166Z","iopub.execute_input":"2022-01-22T14:29:04.264641Z","iopub.status.idle":"2022-01-22T14:29:04.415901Z","shell.execute_reply.started":"2022-01-22T14:29:04.264596Z","shell.execute_reply":"2022-01-22T14:29:04.414878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Progress bar callback","metadata":{}},{"cell_type":"code","source":"class ProgressBarCallback(BaseCallback):\n    \"\"\"\n    Callback for displaying the progress bar in realtime\n    \n    Attributes\n    ----------\n    pbar : tqdm.pbar\n        Progress bar object\n    \"\"\"\n    def __init__(self, pbar):\n        super(ProgressBarCallback, self).__init__()\n        self.pbar = pbar\n\n    def _on_step(self):\n        self.pbar.n = self.num_timesteps\n        self.pbar.update(0)\n\nclass ProgressBarManager(object):\n    \"\"\"\n    Manager for proper initialisation and destruction of the progress callback \n    using the ``with`` block\n    \n    Attributes\n    ----------\n    pbar : tqdm.pbar\n        Progress bar object\n    total_timesteps : int\n        Number of training steps\n    \"\"\"\n    def __init__(self, total_timesteps):\n        self.pbar = None\n        self.total_timesteps = total_timesteps\n        \n    def __enter__(self): \n        self.pbar = tqdm(total = self.total_timesteps)\n            \n        return ProgressBarCallback(self.pbar)\n\n    def __exit__(self, exc_type, exc_val, exc_tb): \n        self.pbar.n = self.total_timesteps\n        self.pbar.update(0)\n        self.pbar.close()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:29:04.418Z","iopub.execute_input":"2022-01-22T14:29:04.418226Z","iopub.status.idle":"2022-01-22T14:29:04.427659Z","shell.execute_reply.started":"2022-01-22T14:29:04.418197Z","shell.execute_reply":"2022-01-22T14:29:04.426997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Create an Agent","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Neural network model","metadata":{}},{"cell_type":"code","source":"class CustomCNN(BaseFeaturesExtractor):\n    \"\"\"\n    Convolutional neural network for extracting features from observations\n    \n    Attributes\n    ----------\n    cnn : torch.nn.Sequential\n        Object representing CNN part of the neural network\n    linear : torch.nn.Sequential\n        Object representing linear part of the neural network\n    \"\"\"\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n                        \n        # Simple, one convolutional layer with batch normalization. Kernel of\n        # size 4 should be good because for detection of 4 marks in row.      \n        self.cnn = nn.Sequential(\n            nn.Conv2d(\n                in_channels = observation_space.shape[0],\n                out_channels = 64,\n                kernel_size = 4,\n                stride = 1\n            ),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n        \n\n        \n        # Compute flattened shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n        \n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, features_dim),\n            nn.BatchNorm1d(features_dim),\n            nn.ReLU()\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Forward propagation\n    \n        Parameters\n        ----------\n        x : numpy.ndarray\n            Array representing an observation taken from environment\n        \n        Returns\n        ------\n        x : torch.Tensor\n            Tensor representing the output of neural network\n        \"\"\"\n        x = self.cnn(x)\n        x = self.linear(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:29:04.428959Z","iopub.execute_input":"2022-01-22T14:29:04.42939Z","iopub.status.idle":"2022-01-22T14:29:04.443868Z","shell.execute_reply.started":"2022-01-22T14:29:04.429359Z","shell.execute_reply":"2022-01-22T14:29:04.442818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3. Creating/loading agent model","metadata":{}},{"cell_type":"code","source":"POLICY_KWARGS = {\n    'features_extractor_class': CustomCNN,\n    'activation_fn':th.nn.ReLU, # Activation function for MlpPolicy part of the network\n    # First number corresponds to output of the shared layer, that later goes to the:\n    # pi - policy network which generates actions\n    # vf - value network which scores provided observations    \n    'net_arch':[64, dict(pi=[32, 16], vf=[32, 16])],\n    # ``features_dim`` corresponds to the number of features that should be outputted\n    #   from the features_extractor\n    'features_extractor_kwargs': dict(features_dim = 768) \n}\n# Path to model zip. Leave as empty string to create a new model.\nMODEL_PATH = ''\n\n# Load or initialize agent\nif MODEL_PATH:\n    print('Loading existing model\\n')\n    agent = PPO.load(\n        path = MODEL_PATH,\n        env = training_env,\n        verbose = 0\n    )\nelse:\n    print('Creating a new model\\n')\n    agent = PPO(\n        policy = 'MlpPolicy',\n        env = training_env,\n        policy_kwargs = POLICY_KWARGS,\n        verbose = 0\n    )\n\n\nprint(agent.policy)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:29:04.446432Z","iopub.execute_input":"2022-01-22T14:29:04.446673Z","iopub.status.idle":"2022-01-22T14:29:04.697215Z","shell.execute_reply.started":"2022-01-22T14:29:04.446638Z","shell.execute_reply":"2022-01-22T14:29:04.696438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. Training","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Training constants\nTOTAL_TIMESTEPS = 100_000\nEVAL_FREQ = 2048 # The policy is updated every 2048 timesteps\nEVAL_EPISODES = 100 # How many episodes the agent should be tested for evaluation\n\nwith ProgressBarManager(TOTAL_TIMESTEPS) as progress_callback:\n    agent.learn(\n        total_timesteps = TOTAL_TIMESTEPS,\n        callback = [\n            progress_callback,\n            EvalCallback(\n                eval_env = training_env,\n                n_eval_episodes = EVAL_EPISODES,\n                best_model_save_path = LOG_DIR,\n                log_path = LOG_DIR,\n                eval_freq = EVAL_FREQ,\n                deterministic = True,\n                render = False\n            )\n        ]\n    )\n\n# Display training result\ndf = load_results(LOG_DIR)['r']\ndf.rolling(window = 1000).mean().plot()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:29:04.698266Z","iopub.execute_input":"2022-01-22T14:29:04.698463Z","iopub.status.idle":"2022-01-22T14:47:16.610367Z","shell.execute_reply.started":"2022-01-22T14:29:04.698438Z","shell.execute_reply":"2022-01-22T14:47:16.6095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3. Neural network agent\nFor testing purposes - unoptimized and not encapsulated (not a valid submission)","metadata":{}},{"cell_type":"code","source":"def trained_nn_agent(observation, config = {'rows': 6, 'columns': 7, 'inarow': 4}):\n    \"\"\"\n    Calculate next move based on neural network prediction\n    \n    Parameters\n    ----------\n    observation: numpy.ndarray\n        A ``numpy.ndarray`` representing empty game board\n    config: dict of {str: int}\n        A dictionary with configuration parameters. Normally, it's passed to the\n        agent by the environment.\n        \n    Returns\n    -------\n    action: int\n        Number representing the column chosen by the agent \n    \"\"\"\n    import numpy as np\n        \n    observation = np.array(\n        observation['board']\n    ).reshape(1, config['rows'], config['columns'])\n    action, info = agent.predict(observation)\n    \n    return int(action)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:47:16.611892Z","iopub.execute_input":"2022-01-22T14:47:16.612709Z","iopub.status.idle":"2022-01-22T14:47:16.620784Z","shell.execute_reply.started":"2022-01-22T14:47:16.612659Z","shell.execute_reply":"2022-01-22T14:47:16.62004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Agent evaluation","metadata":{}},{"cell_type":"markdown","source":"## 5.1. Encapsulated heuristic agent\nA simple rule-based agent using minimax algorithm and custom heuristic weights for benchmarking. Ready for submission (got to around 47 place on leaderboard).","metadata":{}},{"cell_type":"code","source":"def heuristic_agent(observation, config):\n    \"\"\"\n    Calculate next move by built in heuristic prediction. Largely unoptimized in\n    terms of speed.\n    \n    Parameters\n    ----------\n    observation: kaggle_environments.object\n        An object representing the observation of the environment. Mostly used\n        to get the current game board which is a ``numpy.ndarray``.\n    config: dict of {str: int}\n        A dictionary with environment configuration. Normally, it's passed to the\n        agent by the environment.\n        \n    Returns\n    -------\n    action: int\n        Number representing the column chosen by the agent \n    \"\"\"\n    import random, numpy as np\n    \n    # Max depth of game state tree search\n    LOOKAHEAD = 3\n    \n    def drop_piece(board, column, piece, config):\n        next_board = board.copy()\n        \n        for row in range(config.rows - 1, -1, -1):\n            if next_board[row][column] == 0:\n                break\n                \n        next_board[row][column] = piece\n        \n        return next_board\n    \n    def check_window(window, num_discs, piece, config):\n        return (\n            window.count(piece) == num_discs and\n            window.count(0) == config.inarow - num_discs\n        )\n    \n    def count_windows(board, num_discs, piece, config):\n        \"\"\"\n        Count number of windows satisfying the specified heuristic conditions\n\n        Parameters\n        ----------\n        board : numpy.ndarray\n            A ``numpy.ndarray`` representing game board\n        num_discs : int\n            Number representing how many discs in a row should be counted\n        piece : int\n            Number representing which piece the agent is playing\n        config : dict of {str: int}\n            A dictionary with environment configuration \n            \n        Returns\n        -------\n        num_windows : int\n            Number representing how many windows that satisfy the parameters were\n            found in current game board\n        \"\"\"\n        num_windows = 0\n        \n        # Horizontal\n        for row in range(config.rows):\n            for column in range(config.columns - config.inarow - 1):\n                window = list(board[row, column : column + config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n                    \n        # Vertical\n        for row in range(config.rows - config.inarow - 1):\n            row_slice_end = row + config.inarow\n            for column in range(config.columns):\n                window = list(board[row : row_slice_end, column])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n                    \n        # Positive diagonal\n        for row in range(config.rows - config.inarow - 1):\n            row_stop = row + config.inarow\n            for column in range(config.columns - config.inarow - 1):\n                window = list(\n                    board[\n                        range(row, row_stop),\n                        range(column, column + config.inarow)\n                    ]\n                )\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n                    \n        # Negative diagonal\n        for row in range(config.inarow - 1, config.rows):\n            row_stop = row - config.inarow\n            for column in range(config.columns - config.inarow - 1):\n                window = list(\n                    board[\n                        range(row, row_stop, -1),\n                        range(column, column + config.inarow)\n                    ]\n                )\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        \n        return num_windows\n    \n    def get_heuristic(board, piece, config):\n        \"\"\"\n        Calculate the value of a given game board with built in heuristic.\n        The heuristic was chosen arbitrarily.\n\n        Parameters\n        ----------\n        board : numpy.ndarray\n            A ``numpy.ndarray`` representing game board\n        piece : int\n            Number representing which piece the agent is playing\n        config : dict of {str: int}\n            A dictionary with environment configuration \n            \n        Returns\n        -------\n        score : int\n            Number representing the score calculated for a given game board\n        \"\"\"\n        ENEMY_PIECE = piece % 2 + 1\n        \n        num_fours = count_windows(board, 4, piece, config)\n        num_threes = count_windows(board, 3, piece, config)\n        num_twos = count_windows(board, 2, piece, config)\n        num_fours_opp = count_windows(board, 4, ENEMY_PIECE, config)\n        num_threes_opp = count_windows(board, 3, ENEMY_PIECE, config)\n        num_twos_opp = count_windows(board, 2, ENEMY_PIECE, config)\n        \n        score = 1e6 * num_fours + 1e3 * num_threes + 1e2 * num_twos \\\n        - 1e5 * num_fours_opp - 1e4 * num_threes_opp - 1e2 * num_twos_opp \n        \n        return score\n    \n    def score_move(board, column, piece, config, lookahead):\n        \"\"\"\n        Use minimax algorithm to calculate the value of dropping piece in selected\n        column\n\n        Parameters\n        ----------\n        board : numpy.ndarray\n            A ``numpy.ndarray`` representing game board\n        column : int\n            Number representing column\n        piece : int\n            Number representing which piece the agent is playing\n        config : dict of {str: int}\n            Dictionary with environment configuration\n        lookahead : int\n            Number representing max depth to which the game board tree is explored\n            \n        Returns\n        -------\n        score : int\n            Number representing the score calculated for a given game board\n        \"\"\"\n        next_board = drop_piece(board, column, piece, config)\n        score = minimax(next_board, lookahead - 1, False, piece, config)\n        \n        return score\n\n    def is_terminal_window(window, config):\n        return window.count(1) == config.inarow or window.count(2) == config.inarow\n\n    def is_terminal_node(board, config):\n        \"\"\"\n        Check if the current game board finishes the game\n\n        Parameters\n        ----------\n        board : numpy.ndarray\n            A ``numpy.ndarray`` representing game board\n        config : dict of {str: int}\n            A dictionary with environment configuration \n            \n        Returns\n        -------\n        is_terminal : bool\n            Number representing how many windows that satisfy the parameters were\n            found in current game board\n        \"\"\"\n        # Check for draw \n        if list(board[0, :]).count(0) == 0:\n            return True\n        \n        # Check for win: horizontal, vertical, or diagonal\n        # Horizontal \n        for row in range(config.rows):\n            for column in range(config.columns - config.inarow - 1):\n                window = list(board[row, column: column + config.inarow])\n                if is_terminal_window(window, config):\n                    return True\n                \n        # Vertical\n        for row in range(config.rows - config.inarow - 1):\n            row_slice_end = row + config.inarow\n            for column in range(config.columns):\n                window = list(board[row : row_slice_end, column])\n                if is_terminal_window(window, config):\n                    return True\n                \n        # Positive diagonal\n        for row in range(config.rows - config.inarow - 1):\n            row_stop = row + config.inarow\n            for column in range(config.columns - config.inarow - 1):\n                window = list(\n                    board[\n                        range(row, row_stop),\n                        range(column, column + config.inarow)\n                    ]\n                )\n                if is_terminal_window(window, config):\n                    return True\n                \n        # Negative diagonal\n        for row in range(config.inarow - 1, config.rows):\n            row_stop = row - config.inarow\n            for column in range(config.columns - config.inarow - 1):\n                window = list(\n                    board[\n                        range(row, row_stop, -1),\n                        range(column, column + config.inarow)\n                    ]\n                )\n                if is_terminal_window(window, config):\n                    return True\n                \n        # Game board isn't terminal      \n        return False\n\n    # Minimax implementation\n    def minimax(node, depth, isMaximizingPlayer, piece, config):\n        \"\"\"\n        Maximize or minimize the score based \n\n        Parameters\n        ----------\n        node : numpy.ndarray\n            A ``numpy.ndarray`` representing game board in a current node\n        depth : int\n            Number representing tree max tree depth\n        isMaximizingPlayer : bool\n            Flag which determines if the algorithm should maximize or minimize\n        piece : int\n            Number representing which pieces should be maximized or minimized\n        config : dict of {str: int}\n            A dictionary with environment configuration\n            \n        Returns\n        -------\n        value : int\n            Number representing how many windows that satisfy the parameters were\n            found in current game board\n        \"\"\"\n        IS_TERMINAL = is_terminal_node(node, config)\n        VALID_MOVES = [col for col in range(config.columns) if node[0][col] == 0]\n        \n        if depth == 0 or IS_TERMINAL:\n            return get_heuristic(node, piece, config)\n        \n        value = 0\n        \n        if isMaximizingPlayer:\n            value = -np.Inf\n            for column in VALID_MOVES:\n                child = drop_piece(node, column, piece, config)\n                value = max(value, minimax(child, depth - 1, False, piece, config))        \n        else:\n            value = np.Inf\n            for column in VALID_MOVES:\n                child = drop_piece(node, column, piece % 2 + 1, config)\n                value = min(value, minimax(child, depth - 1, True, piece, config))\n        \n        return value\n    \n    # Get list of valid moves\n    valid_moves = [\n        col for col in range(config.columns) if observation.board[col] == 0\n    ]\n    \n    # Convert the board to a 2D board\n    board = np.asarray(observation.board).reshape(config.rows, config.columns)\n    \n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(\n        zip(\n            valid_moves,\n            [\n                score_move(board, column, observation.mark, config, LOOKAHEAD) \\\n                for column in valid_moves\n            ]\n        )\n    )\n    \n    # Get a list of columns (moves) that maximize the heuristic\n    max_columns = [\n        key for key in scores.keys() if scores[key] == max(scores.values())\n    ]\n    \n    # Select at random from the maximizing columns\n    return random.choice(max_columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:47:16.622473Z","iopub.execute_input":"2022-01-22T14:47:16.623062Z","iopub.status.idle":"2022-01-22T14:47:16.668853Z","shell.execute_reply.started":"2022-01-22T14:47:16.623015Z","shell.execute_reply":"2022-01-22T14:47:16.668018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2. Agent validation\nCheck agent predictions in a single game with console output","metadata":{}},{"cell_type":"code","source":"env = make(\"connectx\", debug = 1)\n\n# Two agents play one game round\nenv.run([trained_nn_agent, heuristic_agent])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:47:16.671455Z","iopub.execute_input":"2022-01-22T14:47:16.67241Z","iopub.status.idle":"2022-01-22T14:47:26.961594Z","shell.execute_reply.started":"2022-01-22T14:47:16.672364Z","shell.execute_reply":"2022-01-22T14:47:26.959305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3. Evaluate an Agent\nEvaluate the agent performance against selected agents. Kaggle also provides two agents:\n- 'random' - does a random valid move\n- 'negamax' - a negamax agent with 4 moves lookahead\n\nThe heuristic agents are unfortunately much better. RL agent would need to use a bit more sophisticated techniques and train longer to defeat them.","metadata":{}},{"cell_type":"code","source":"def evaluate_agent(\n    agent1,\n    agent2,\n    config = {'rows': 6, 'columns': 7, 'inarow': 4},\n    num_episodes = 100\n):\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\n        environment = \"connectx\",\n        agents = [agent1, agent2],\n        configuration = config,\n        num_episodes = num_episodes // 2\n    )\n\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [\n        [b,a] for [a,b] in evaluate(\n            environment = \"connectx\",\n            agents = [agent2, agent1],\n            configuration = config,\n            num_episodes = num_episodes -  num_episodes // 2\n        )\n    ]\n    \n    print(\n        \"Agent 1 Win Percentage:\",\n        np.round(outcomes.count([1,-1])/len(outcomes), 2)\n    )\n    print(\n        \"Agent 2 Win Percentage:\",\n        np.round(outcomes.count([-1,1])/len(outcomes), 2)\n    )\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n\nprint('Trained agent vs Random')\nevaluate_agent(trained_nn_agent, 'random')\n\nprint('\\n')\n\n# This may take a while\nprint('Trained agent vs Heuristic')\nevaluate_agent(trained_nn_agent, heuristic_agent)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-22T14:47:26.962979Z","iopub.execute_input":"2022-01-22T14:47:26.96327Z","iopub.status.idle":"2022-01-22T14:47:31.500764Z","shell.execute_reply.started":"2022-01-22T14:47:26.963233Z","shell.execute_reply":"2022-01-22T14:47:31.499935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Create submission\nFor more information see:\nhttps://www.kaggle.com/c/connectx","metadata":{}},{"cell_type":"markdown","source":"## 6.1. Create an encapsulated agent function","metadata":{}},{"cell_type":"code","source":"%%writefile submission.py\n\ndef submission_agent(observation, config):\n    import numpy as np\n    import torch as th\n    from torch import nn as nn\n    from torch import tensor\n    \n    class ActorCriticPolicy(nn.Module):\n        \"\"\"\n        Neural network for a actor critic method. The feature extractor part is the\n        same as in ``CustomCNN``, but there are additional parts that were previously\n        provided by the agent policy. Required for encapsulation.\n\n        Attributes\n        ----------\n        features_extractor : torch.nn.Sequential\n            Object representing CNN part of the neural network\n        shared_net : torch.nn.Sequential\n            Object representing part of the neural network that was transforming the\n            ``features_extractor`` into policy and value networks (provided by MlpPolicy)\n        policy_net : torch.nn.Sequential\n            Object representing part of the neural network that was transforming the\n            output of ``shared_net`` into policy \n            (doesn't need to be related to the action space)\n        action_net : torch.nn.Sequential\n            Object representing part of the neural network that was transforming the\n            output of ``policy_net`` into actions (related to action space)\n        \"\"\"\n        def __init__(self):\n            super(ActorCriticPolicy, self).__init__()\n            \n            self.features_extractor = nn.Sequential(\n                nn.Conv2d(\n                    in_channels = 1,\n                    out_channels = 64,\n                    kernel_size = 4,\n                    stride = 1\n                ),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.Flatten(),\n                nn.Linear(768, 768),\n                nn.BatchNorm1d(768),\n                nn.ReLU()\n            )\n            \n            self.shared_net = nn.Sequential(\n                nn.Linear(768, 64),\n                nn.ReLU()\n            )\n            \n            self.policy_net = nn.Sequential(\n                nn.Linear(64, 32),\n                nn.ReLU(),\n                nn.Linear(32, 16),\n                nn.ReLU()\n            )\n            \n            self.action_net = nn.Sequential(\n                nn.Linear(16, 7),\n                nn.ReLU()\n            )\n\n        def forward(self, x):\n            x = self.features_extractor(x)\n            x = self.shared_net(x)\n            x = self.policy_net(x)\n            x = self.action_net(x)\n            x = x.argmax() # Take action with highest weight\n            \n            return x","metadata":{"execution":{"iopub.status.busy":"2022-01-22T15:06:46.036859Z","iopub.execute_input":"2022-01-22T15:06:46.037377Z","iopub.status.idle":"2022-01-22T15:06:46.046216Z","shell.execute_reply.started":"2022-01-22T15:06:46.037321Z","shell.execute_reply":"2022-01-22T15:06:46.045383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2. Append neural network weights to the file","metadata":{}},{"cell_type":"code","source":"print('Trained model network:\\n')\nprint(agent.policy.state_dict().keys())\n\nagent_path = 'submission.py'\n\nstate_dict = agent.policy.to('cpu').state_dict()\n\n# Reassign the neural network parameters to the attributes of ActorCriticPolicy class.\n# Activation functions and flatten is skipped.\nstate_dict = {\n    'features_extractor.0.weight': state_dict['features_extractor.cnn.0.weight'],\n    'features_extractor.0.bias': state_dict['features_extractor.cnn.0.bias'],\n    'features_extractor.1.weight': state_dict['features_extractor.cnn.1.weight'],\n    'features_extractor.1.bias': state_dict['features_extractor.cnn.1.bias'],\n    'features_extractor.1.running_mean': state_dict[\n        'features_extractor.cnn.1.running_mean'\n    ],\n    'features_extractor.1.running_var': state_dict[\n        'features_extractor.cnn.1.running_var'\n    ],\n    'features_extractor.1.num_batches_tracked': state_dict[\n        'features_extractor.cnn.1.num_batches_tracked'\n    ],\n    'features_extractor.4.weight': state_dict['features_extractor.linear.0.weight'],\n    'features_extractor.4.bias': state_dict['features_extractor.linear.0.bias'],\n    'features_extractor.5.weight': state_dict['features_extractor.linear.1.weight'],\n    'features_extractor.5.bias': state_dict['features_extractor.linear.1.bias'],\n    'features_extractor.5.running_mean': state_dict[\n        'features_extractor.linear.1.running_mean'\n    ],\n    'features_extractor.5.running_var': state_dict[\n        'features_extractor.linear.1.running_var'\n    ],\n    'features_extractor.5.num_batches_tracked': state_dict[\n        'features_extractor.linear.1.num_batches_tracked'\n    ],\n    \n    'shared_net.0.weight': state_dict['mlp_extractor.shared_net.0.weight'],\n    'shared_net.0.bias': state_dict['mlp_extractor.shared_net.0.bias'],\n    \n    'policy_net.0.weight': state_dict['mlp_extractor.policy_net.0.weight'],\n    'policy_net.0.bias': state_dict['mlp_extractor.policy_net.0.bias'],\n    'policy_net.2.weight': state_dict['mlp_extractor.policy_net.2.weight'],\n    'policy_net.2.bias': state_dict['mlp_extractor.policy_net.2.bias'],\n    \n    'action_net.0.weight': state_dict['action_net.weight'],\n    'action_net.0.bias': state_dict['action_net.bias'],\n}\n\nwith open(agent_path, mode='a') as file:\n    file.write(f'    state_dict = {state_dict}\\n')\n    print('\\nAppending model weights successful!')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T15:06:48.847311Z","iopub.execute_input":"2022-01-22T15:06:48.849976Z","iopub.status.idle":"2022-01-22T15:06:56.113566Z","shell.execute_reply.started":"2022-01-22T15:06:48.849907Z","shell.execute_reply":"2022-01-22T15:06:56.112619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3. Append the ``move`` function\nSteps necessary for agent to make a move","metadata":{}},{"cell_type":"code","source":"%%writefile -a submission.py\n\n    model = ActorCriticPolicy()\n    # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.float\n    model = model.float()\n    # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict\n    model.load_state_dict(state_dict)\n    # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to \n    # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cpu\n    model = model.to('cpu')\n    # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval\n    model = model.eval()\n    \n    observation = tensor(observation['board']).reshape(\n        1, 1, config.rows, config.columns\n    ).float()\n    action = model(observation)\n    \n    return int(action)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T15:06:56.114871Z","iopub.execute_input":"2022-01-22T15:06:56.115082Z","iopub.status.idle":"2022-01-22T15:06:56.121494Z","shell.execute_reply.started":"2022-01-22T15:06:56.115055Z","shell.execute_reply":"2022-01-22T15:06:56.120545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Validate Submission","metadata":{}},{"cell_type":"markdown","source":"## 7.1. Play against itself\nPlay the submission agent against itself. This is the first episode the competition will run to weed out erroneous agents, which will roughly verify if agent is fully encapsulated and can be run remotely.","metadata":{}},{"cell_type":"code","source":"with open(agent_path, mode = 'r') as file:\n    source = file.read()\n    exec(source)\n    \nenv = make(\"connectx\", debug = True)\n\nenv.run([submission_agent, submission_agent])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T15:07:03.316811Z","iopub.execute_input":"2022-01-22T15:07:03.317087Z","iopub.status.idle":"2022-01-22T15:07:12.332319Z","shell.execute_reply.started":"2022-01-22T15:07:03.317058Z","shell.execute_reply":"2022-01-22T15:07:12.330765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2. Evaluate agent\nTo make sure that the neural network weight are assigned correctly","metadata":{}},{"cell_type":"code","source":"evaluate_agent(agent1 = submission_agent, agent2 = 'random')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T14:47:48.149444Z","iopub.execute_input":"2022-01-22T14:47:48.149671Z","iopub.status.idle":"2022-01-22T14:49:55.66136Z","shell.execute_reply.started":"2022-01-22T14:47:48.149643Z","shell.execute_reply":"2022-01-22T14:49:55.660503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit to Competition\n\n1. Commit this kernel.\n2. View the commited version.\n3. Go to \"Data\" section and find submission.py file.\n4. Click \"Submit to Competition\"\n5. Go to [My Submissions](https://kaggle.com/c/connectx/submissions) to view your score and episodes being played.","metadata":{}}]}